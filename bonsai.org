#+title: Bonsai
#+subtitle: Statistical Benchmarking in J
#+OPTIONS: author:nil num:nil
#+HTML_HEAD: <link rel="stylesheet" href="../format/css.css" />
#+HTML_HEAD: <link rel="icon" type="image/png" href="../images/icon.png" />

This is a *~J~* program which does some statistical analysis on
computer benchmark results. This is also a collection of notes so that
I may recall what I learned while writing this script. 

I mostly wanted a way to know which changes I was making to ~J~
programs were actually effective. The built in time function ~x 6!:2
y~ which averages the time taken to run ~y~ ~x~ times is
unsatisfactory for this purpose due to the performance characteristics
of ~J~ programs, which typically run slower on the first few trials
and warm up a as they go. A consequence is for light programs ~y~ a
large ~x~ is needed before the results converge. The averaging also
hides variability and range of the performance of ~y~.

The method employed here is bootstrapping, which enables the
estimation of statistical parameters on nonparametric samples. I got
the sense of what to look for from haskell's [[https://hackage.haskell.org/package/criterion][criterion]] package, and
learned the material for how to implement it through Efron and
Hastie's textbook [[https://web.stanford.edu/~hastie/CASI/][Computer Age Statistical Inference]]. See chapters 10
and 11 from there to get the original material.

* Background

We take a sample $x = (x_i)$ of benchmark results, which we (slightly
dubiously) assume iid from some unknown distribution. From that we
compute various statistics on that sample from corresponding
algorithms $\hat\theta = s(x)$ in order to understand the results.

Some of these benchmarks will be expensive to run and in general it
won't be possible to gather a large sample. To that end, the process
of statistical bootsrapping allows us to extrapolate better estimates
on the desired statistics through resampling uniformly from
$x$. Moreover, this allows us to calculate standard errors and to
attach confidence intervals on these computed statistics. And for our
purposes, perhpas the most crucial aspect is that nothing need be
assumed or known about the underlying distrubiton and that the
computations are automatic.

** Bootstrapping

A single bootstrap resample of the original sample $x$ is $x^* =
(x_i^*)$ where the $x_i*$ are drawn uniformly from $x$ and
$n=|x|=|x^*|$. This resampling is carried out $B$ times, comprising
the bootstrapped resample on which we compute statistics
$\hat\theta^{*b}$. And from that, statistics such as 

$$\hat {\text{se}} = \sqrt{\frac {\sum_b \big(\hat\theta^{*b} -
\hat\theta^{*\cdot}\big)^2}{B-1}}$$

can be computed. Basically, the process is getting a sample $x$ from
some unknown distribution $F$, and computing a statistic $\hat\theta$
on it. Then, better estimates of $\hat\theta$ come from resampling
$x^*$ from the emperical distribution $\hat F$ which assigns
probability $\frac{1}{n}$ to each $x_i$ from $x$ then calculating
$\hat\theta^*$. The key to this process is that $\hat F \rightarrow F$
as $n \rightarrow \infty$, and that for any $n$, $\hat F$ maximizes
the probability of having observed $x$ from $F$, whatever $F$ may
actually be. In other words, $\hat F$ is the nonparametric maximum
likelihood estimator for $F$.

The final step in the above process is akin to running another
algorithm $\text{Sd}(\hat F) = \hat{\text{se}}$ on the bootstrap
resample and is called the _ideal bootstrap estimate_ of the standard
error. We can, however, do better inference and construct confidence
intervals on the $\hat\theta^*$. 

Before describing the bootstrap confidence calculations, a diversion
on configuration.

* Initial Sample and Configuration

The basic configuration is the amount of time alloted for the initial
benchmarks, the minimum number of runs, and the maximum number
runs. Additional configuration includes the target coverage for
confidence intervals and the number of bootstrap trials.

#+name: configuration
#+begin_src j :exports code
bs_1rn  =: 1      NB. time alloted (upper bound on)
bs_n_lo =: 5      NB. minimum sample
bs_n_hi =: 1000   NB. maximum sample
bs_a    =: 0.1    NB. coverage
bs_B    =: 2000   NB. bootstrap resample
#+end_src

We gather an initial sample by first running the program once and
estimate then estimate how many more times to sample it based on how
long it took (~bs_1rn % 6!:2~):

#+name: sampling
#+begin_src j :session :exports code
NB. monad taking prgram y to run a number of times based on configuration
dobench=: 6!:2"1@(# ,:)~ (bs_n_lo >. bs_n_hi <. [: <. bs_1rn % 6!:2)
NB. u is parameter, n is bootstrap B, y is sample
dobootstrap=: 2 : 'u"1 y {~ ? n # ,: $~ #y'
#+end_src

#+RESULTS: dobench

* Statistical Algorithms

Many of these come from J's ~stats/base~ addon, but are included for
fun and for completeness.

#+name: basic-statistics-algorithms
#+begin_src j :session :exports code
mean   =: +/ % #         NB. is what it is
dev    =: -"_1 _ mean    NB. deviation from mean (of possibly vector valued sample)
ssdev  =: +/@:*:@:dev    NB. sum of squares of devation
var    =: ssdev % <:@#   NB. unbiased variance
stddev =: %:@var         NB. corrected sample standard deviation
#+end_src

#+RESULTS: basic-statistics-algorithms

These correspond in normal math notation to

$$\bar x = \sum_i \frac{x_i}{n}$$
$$\text{Var} (x) = \frac{\sum_i (x_i - \bar x)^2}{n-1}$$
$$s = \sqrt \frac{\sum_i (x_i - \bar x)^2}{n-1}$$

Discrete percentiles and quantiles are not in J's stats addon and can
be computed as follows:

#+name: quantile
#+begin_src j :session :exports code
discrete_cdf=: 4 : 0
ws=. (%+/)"1 -. | xs -"0 1 is=. (<.,>.)"0 xs=. x * <:#y
ws (+/"1 @: *) is { /:~ y
)

quantile =: discrete_cdf :. (+/ @: (<:/~) % #@])

meadian =: 0.5 & quantile
#+end_src

#+RESULTS: quantile

The local variables ~is~ and ~ws~ are used to interpolate between
values at neighboring indices so that for example ~0.5 discrete_cdf 0
3~ and ~median 0 3~ agree and are both ~1.5~. Declaring quantile as a
function with obverse is cute but technically not valid. The delcared
obverse counts how many elements of ~y~ are less than or equal to ~x~.

* Bootstrapping Confidence

Corresponds to Chapter 11 of casi textbook. Throughout, goal is to
estimate the unseen statistic $\theta$ from the bootstrap resample
$\hat\theta^*$

** Standard Interval

The simplest but least accurate way of stamping a condience interval
on the resampled statistics $\hat\theta^*$ is by taking the
bootstrapped standard error and asking for coverage based on the
normal distribution cdf.

#+name: standard-interval
#+begin_src j :session :exports code
NB. monad producing adverb where u is statistic and y is sample.
bssi=: 1 : 0
  samp=. (u dobootstrap bs_B) y
  (mean samp) -`+`:0 (stddev samp) * qnorm -. -: bs_a
)
#+end_src

#+RESULTS: standard-interval

In other words for 95% coverage the estimate for $\theta$ is inside
interval $\hat \theta \pm 1.96 \cdot \hat {\text{se}}$. 1.96 comes
from cdf of standard normal distribution $\Phi^{-1}(0.975)$. The 0.975
comes from $1 - \frac{\alpha}{2}$ and our $\alpha$ is configured
through the variable ~bs_a~.

** Percentile Interval

The next best way to go is to use percentiles on the emperical
resamples to find our confidence.

#+name: percentile
#+begin_src j :session :exports code
NB. monad producing adverb where u is statistic and y is sample.
bspi=: 1 : 0
  ((,-.) -: bs_a) quantile (u dobootstrap bs_B) y
)
#+end_src

In other words, we estimate $\theta$ from the bootstrap cdf $\hat F$,
and get the interval $\hat F^{-1}[\frac{\alpha}{2},1 -
\frac{\alpha}{2}]$. In J the base interval is cutely calculated by
hooking ~(,-.) -: bs_a~.

** Bias-corrected Percentile Interval

The resamples may skew more heavily to one side or the other of $\hat
\theta$. To correct for this, we look at the percentile of the it in
the resample then derive the bounds on the confidence interval by
mapping through the standard normal cdf $\Phi$ getting the desired
coverage and then calculating percentiles.

#+name: bias-percentile
#+begin_src j :session :exports code
NB. monad producing adverb where u is statistic and y is sample.
bsbc=: 1 : 0
  that =. mean samp=. u dobootstrap bs_B y
  z0=. qnorm p0=. that quantile^:_1 samp
  I=. pnorm (+: z0) + (qnorm (,-.) -: bs_a)
  ({.,(mean samp),{:) I quantile samp
)
#+end_src

#+RESULTS: bias-percentile

The above corresponds to
$$p_0=\frac{\#\{\hat\theta^{*b} \le \hat \theta^*\}}{B}$$
$$z_0=\Phi^{-1} (p_0)$$ $$\hat\theta_{\text{BC}}[\alpha] = \hat F^{-1}
[\Phi (2\cdot z_0 + z^{(\alpha)})]$$

When the bootstrap resamples are median unbiased (ie $p_0 = 0.5$) then
$z_0=0$ and this agrees with the simple percentile interval.

** Bias-corrected and Accelerated Percentile Interval

#+name: bias-and-accelerated
#+begin_src j :session :exports code
NB. monad producing adverb where u is statistic and y is sample.
bsbca=: 1 : 0
  thati=. (- mean) 1 u \. y
  ahat=. 1r6 * (+/thati^3) % (+/*:thati)^3r2
  that=. mean samp=. u dobootstrap bs_B y
  z0=. qnorm that quantile^:_1 samp
  zb=. qnorm -. -: bs_a
  zbh=. z0 + (z0+zb) % 1 - ahat * z0+zb
  za=. qnorm -: bs_a
  zah=. z0 + (z0+za) % 1 - ahat * z0+za
  ({.,that,{:) (pnorm zah,zbh) quantile samp
)
#+end_src

#+RESULTS: bias-and-accelerated

* Regression

J programs don't tend to have much overhead, but this is a nice idea
from criterion. One way to estimate the performance of a program is to
do a linear regression on the sample. Presumably the overhead will be
captured in the constant term, giving a clearer picture of typical
execution times. Here, we sum of the execution times to get ~n~
snapshots of performance.

#+name: regression
#+begin_src j :session :exports both
regress_bench=: +/\ %. 1 ,. i.@#
rsquare_bench=: 3 : 0
  v=. 1,.i.#y
  d=. +/\ y
  b=. d %. v
  k=. <:{:$v
  n=. $d
  sst=. +/*:d-(+/d) % #d
  sse=. +/*:d-v +/ .* b
  mse=. sse%n->:k
  seb=. %:({.mse)*(<0 1)|:%.(|:v) +/ .* v
  ssr=. sst-sse
  msr=. ssr%k
  rsq=. ssr%sst
  rsq
)
#+end_src

* Description

We default to the most sophisticated confidence measurement ~bsbca~
and estimate some descriptive statistics in ~summarize~. This is a
first draft I'd like to build out some functionality to compare
benchmarks as well as output some kind of plots. Verb ~bonsai~ takes a
J sentence, gets a sample, then runs the analysis. ~summarize~ is
split out so that benchmark results gotten somehow else can also be
subject to this analysis.

#+name: analysis
#+begin_src j :session :exports both
NB. use bs bias corrected accelerated by default
bs_est =: bsbca

NB. report some descriptive statistics about a single vector y of
NB. benchmark results.
bs_summarize =: 3 : 0
  samp=. y

  xbarc=. mean bs_est samp
  sdevc=. stddev bs_est samp
  regac=. ({:@regress_bench) bs_est samp
  rsqrc=. rsquare_bench bs_est samp
  skwnc=. skewness bs_est samp
  kurtc=. kurtosis bs_est samp
  ests=. <"0 regac , rsqrc , xbarc , sdevc , skwnc ,: kurtc
  ests=. (;: 'lower estimate upper') , ests

  rows=. ('N = ',":#samp);'ols';('R',u:16b00b2);'mean';'stddev';'skewness';'kurtosis'
  rows ,. ests
)

NB. run bencharks then report
bonsai=: bs_summarize @ dobench
#+end_src

* Final Program

#+begin_src j :session :tangle bonsai.ijs :noweb yes
load 'plot stats/base stats/distribs'

<<configuration>>

<<sampling>>

<<quantile>>

<<standard-interval>>

<<percentile>>

<<bias-percentile>>

<<bias-and-accelerated>>

<<regression>>

<<analysis>>
#+end_src
